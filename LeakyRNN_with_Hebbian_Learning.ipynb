{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCpotY/vTEF46CtpnCsn8h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrisGeorge24044/Cognitive_Artificial_Intelligence/blob/main/LeakyRNN_with_Hebbian_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hebbian Learning Rule:\n",
        "- Neurons that fire together, wire together\n",
        "\n",
        "    Δw<sub>ij</sub> = η ⋅ x<sub>i</sub> ⋅ y<sub>j</sub>\n",
        "\n",
        "- w<sub>ij</sub> = Weight which connects the pre- and post-synaptic neurons together\n",
        "- x<sub>i</sub> = pre-synaptic neuron activity\n",
        "- y<sub>j</sub> = post-synaptic neuron activity\n",
        "- η = Learning Rate\n"
      ],
      "metadata": {
        "id": "IkixReisQkoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neurogym\n",
        "!pip install gym==0.25.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI5dWlYkQrTR",
        "outputId": "1fd80cd5-b896-4e2b-c472-906c71254b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neurogym in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neurogym) (1.26.4)\n",
            "Collecting gym<0.25,>=0.20.0 (from neurogym)\n",
            "  Using cached gym-0.24.1-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from neurogym) (3.8.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->neurogym) (1.16.0)\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.1\n",
            "    Uninstalling gym-0.25.1:\n",
            "      Successfully uninstalled gym-0.25.1\n",
            "Successfully installed gym-0.24.1\n",
            "Collecting gym==0.25.1\n",
            "  Using cached gym-0.25.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (0.0.8)\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.24.1\n",
            "    Uninstalling gym-0.24.1:\n",
            "      Successfully uninstalled gym-0.24.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "neurogym 0.0.2 requires gym<0.25,>=0.20.0, but you have gym 0.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "t5pGpjUCVlFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeakyRNN Class defines the Leaky RNN and sets parameters."
      ],
      "metadata": {
        "id": "syxcL33ygO0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeakyRNN(nn.Module):\n",
        "  # Function applies the hebbian learning rule\n",
        "  def hebbian_learning_rule(self, input, hidden, lr = 0.0001):\n",
        "    batch_size = input.size(0)\n",
        "\n",
        "    # Add dimension for input and hidden\n",
        "    input = input.view(batch_size, 1, -1)\n",
        "    hidden = hidden.view(batch_size, -1, 1)\n",
        "\n",
        "    # Computes weight change for input to hidden state\n",
        "    # torch.bmm performs batch multiplication\n",
        "    # Creates (batch_size, hidden_size, input_size)\n",
        "    delta_w_i = lr * torch.bmm(hidden, input)\n",
        "    # Computes weight change for hidden to hidden state\n",
        "    # Creates (batch_size, hidden_size, hidden_size)\n",
        "    delta_w_j = lr * torch.bmm(hidden, hidden.transpose(1, 2))\n",
        "\n",
        "    # Weight changes are added together to update the weight matrices\n",
        "    delta_w_i_sum = delta_w_i.sum(dim=0)\n",
        "    delta_w_j_sum = delta_w_j.sum(dim=0)\n",
        "\n",
        "    # scale factor controls the impact of the learning rule and ensures that\n",
        "    # weights do not increase too quickly as this causes instability\n",
        "    hebbian_scale_factor = 0.001\n",
        "\n",
        "    with torch.no_grad():\n",
        "        self.input2hidden.weight += hebbian_scale_factor * delta_w_i_sum\n",
        "        self.hidden2hidden.weight += hebbian_scale_factor * delta_w_j_sum\n",
        "\n",
        "        self.input2hidden.weight.data = self.input2hidden.weight.data / (torch.norm(self.input2hidden.weight.data) + 1e-6)\n",
        "        self.hidden2hidden.weight.data = self.hidden2hidden.weight.data / (torch.norm(self.hidden2hidden.weight.data) + 1e-6)\n",
        "\n",
        "        # Check for NaN in weights\n",
        "    if torch.isnan(self.input2hidden.weight).any() or torch.isnan(self.hidden2hidden.weight).any():\n",
        "        print(\"NaN detected in weights!\")\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dt = None):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.tau = 100\n",
        "    # Sets alpha as a function of the time step, dt, and the time constant, tau\n",
        "    if dt is None:\n",
        "      alpha = 1\n",
        "    else:\n",
        "      alpha = dt/self.tau\n",
        "    self.alpha = alpha\n",
        "    # Defines linear layers for the input to hidden and hidden to hidden\n",
        "    # connections\n",
        "    self.input2hidden = nn.Linear(input_size, hidden_size)\n",
        "    self.hidden2hidden = nn.Linear(hidden_size, hidden_size)\n",
        "  # init_hidden function initialises the hidden state as a matrix of zeros\n",
        "  def init_hidden(self, input_shape):\n",
        "    batch_size = input_shape[1]\n",
        "    return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "  # Recurrence function updates the hidden state\n",
        "  def recurrence(self, input, hidden):\n",
        "    self.hebbian_learning_rule(input, hidden)\n",
        "    # ReLU non-linear activation function is used to calculate the new hidden\n",
        "    # state using the input and recurrent weight matrices\n",
        "    h_new = torch.relu(self.input2hidden(input) + self.hidden2hidden(hidden))\n",
        "    # Leaky integration is applied with alpha deciding how much of the last\n",
        "    # state is retained\n",
        "    h_new = hidden * (1 - self.alpha) + h_new * self.alpha\n",
        "    return h_new\n",
        "\n",
        "   # Forward function - process the sequence for every time step\n",
        "  def forward(self, input, hidden = None):\n",
        "    # If there is no hidden state, then it is initialised here\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(input.shape).to(input.device)\n",
        "\n",
        "      output = []\n",
        "      # Creates a loop for every time step\n",
        "      steps = range(input.size(0))\n",
        "      for i in steps:\n",
        "        # Updates the hidden state using the recurrence function\n",
        "        hidden = self.recurrence(input[i], hidden)\n",
        "        output.append(hidden)\n",
        "\n",
        "      # Stacks all outputs in one dimension\n",
        "      output = torch.stack(output, dim = 0)\n",
        "      return output, hidden"
      ],
      "metadata": {
        "id": "FGX28vPhVqS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNNet Class creates a fully connected layer which converts the hidden states into final predictions."
      ],
      "metadata": {
        "id": "IpMh2Ma8gaFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
        "    super().__init__()\n",
        "    # The LeakyRNN class is instantiated\n",
        "    self.rnn = LeakyRNN(input_size, hidden_size, **kwargs)\n",
        "    # A fully connected layer is created and applied as the last step of the\n",
        "    # program to obtain predictions\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Forward function processes the input through the Leaky RNN and the FC layer\n",
        "  def forward(self, x):\n",
        "    rnn_output, _ = self.rnn(x)\n",
        "    # FC layer is applied to obtain the predictions for every time step\n",
        "    out = self.fc(rnn_output)\n",
        "    return out, rnn_output"
      ],
      "metadata": {
        "id": "Fegx423sV2oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import neurogym as ngym\n",
        "\n",
        "# ContextDecisionMaking-v0 task is selected from neurogym\n",
        "task_name = 'ContextDecisionMaking-v0'\n",
        "# Time step and stimulus length are defined\n",
        "kwargs = {'dt': 20, 'timing': {'stimulus': 1500}}"
      ],
      "metadata": {
        "id": "4bqec8_KV40V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The sequence length (Num of time steps for each input) and batch size are defined\n",
        "seq_len = 150\n",
        "batch_size = 24\n",
        "\n",
        "# Dataset is created with ngym.Dataset\n",
        "dataset = ngym.Dataset(task_name, env_kwargs=kwargs, batch_size=batch_size, seq_len=seq_len)\n",
        "env = dataset.env\n",
        "\n",
        "# Inputs and targets are taken from the dataset\n",
        "inputs, target = dataset()\n",
        "inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "\n",
        "# Input and output size are defined by the neurogym environment\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "\n",
        "print(inputs.shape)\n",
        "print(target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ1qCJT6V61B",
        "outputId": "dc8ab0a3-296f-4988-dfdd-f9c03bbee5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([150, 24, 7])\n",
            "(150, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 192\n",
        "\n",
        "net = RNNNet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dt=env.dt)\n",
        "print(net)\n",
        "\n",
        "def train_model_hebbian(net, dataset, epoch_num = 10):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(epoch_num):\n",
        "    for i in range(epoch_num):\n",
        "      inputs, labels = dataset()\n",
        "      inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "      labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
        "\n",
        "      hidden = None\n",
        "      output, hidden = net.rnn(inputs, hidden)\n",
        "      output = net.fc(output)\n",
        "\n",
        "      loss = criterion(output.view(-1, output.size(-1)), labels)\n",
        "\n",
        "      print(f\"Epoch num: {epoch + 1}, Step num: {i + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "net = train_model_hebbian(net, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mLTK3agV7Bs",
        "outputId": "dc00b963-34ac-4ae4-8a14-bac1a23805b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNNet(\n",
            "  (rnn): LeakyRNN(\n",
            "    (input2hidden): Linear(in_features=7, out_features=192, bias=True)\n",
            "    (hidden2hidden): Linear(in_features=192, out_features=192, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=192, out_features=3, bias=True)\n",
            ")\n",
            "Epoch num: 1, Step num: 1, Loss: 1.1536469459533691\n",
            "Epoch num: 1, Step num: 2, Loss: 1.1537741422653198\n",
            "Epoch num: 1, Step num: 3, Loss: 1.1537294387817383\n",
            "Epoch num: 1, Step num: 4, Loss: 1.1532005071640015\n",
            "Epoch num: 1, Step num: 5, Loss: 1.151999592781067\n",
            "Epoch num: 1, Step num: 6, Loss: 1.1540336608886719\n",
            "Epoch num: 1, Step num: 7, Loss: 1.1531153917312622\n",
            "Epoch num: 1, Step num: 8, Loss: 1.1525181531906128\n",
            "Epoch num: 1, Step num: 9, Loss: 1.1537060737609863\n",
            "Epoch num: 1, Step num: 10, Loss: 1.15753173828125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Phase:"
      ],
      "metadata": {
        "id": "l36eyeCGgmkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment is reset after training\n",
        "env = dataset.env\n",
        "env.reset(no_step=True)\n",
        "\n",
        "# Performance is tracked\n",
        "perf = 0\n",
        "# Dictionaries to store activity and information for each trial are created\n",
        "activity_dict = {}\n",
        "trial_infos = {}\n",
        "\n",
        "# Number of trials is set to 200\n",
        "num_trial = 200\n",
        "\n",
        "# For loop is created to run the model for 200 trials\n",
        "for i in range(num_trial):\n",
        "  # Starts a trial\n",
        "  trial_info = env.new_trial()\n",
        "  # defines the observations and ground truths for each trial\n",
        "  ob, gt = env.ob, env.gt\n",
        "  # Converts observations to batch dimensions\n",
        "  inputs = torch.from_numpy(ob[:,np.newaxis, :]).type(torch.float)\n",
        "\n",
        "  # Takes the predictions from the model\n",
        "  action_pred, rnn_activity = net(inputs)\n",
        "\n",
        "  # Converts the predictions to a numpy array and selects the action predicted\n",
        "  # by the model (the largest value)\n",
        "  action_pred = action_pred.detach().numpy()[:, 0, :]\n",
        "  choice = np.argmax(action_pred[-1, :])\n",
        "  correct = choice == gt[-1]\n",
        "\n",
        "  # Stores the activity and trial info in dictionaries\n",
        "  rnn_activity = rnn_activity[:, 0, :].detach().numpy()\n",
        "  activity_dict[i] = rnn_activity\n",
        "  trial_infos[i] = trial_info\n",
        "  trial_infos[i].update({'choice': choice, 'correct': correct})\n",
        "\n",
        "  # Tracks the performance\n",
        "  if correct:\n",
        "    perf += 1\n",
        "\n",
        "for i in range(20):\n",
        "  print('Trial', i + 1, trial_infos[i])\n",
        "\n",
        "print(f\"Average performance = {np.mean([val['correct'] for val in trial_infos.values()])}\")\n",
        "print(f\"Performance: {perf}/{num_trial}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y3kpc5BV7J_",
        "outputId": "00ce4adf-a761-4e07-f77a-c0ae294c4edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 {'ground_truth': 1, 'other_choice': 2, 'context': 1, 'coh_0': 50, 'coh_1': 5, 'choice': 2, 'correct': False}\n",
            "Trial 2 {'ground_truth': 2, 'other_choice': 1, 'context': 1, 'coh_0': 15, 'coh_1': 5, 'choice': 2, 'correct': True}\n",
            "Trial 3 {'ground_truth': 2, 'other_choice': 1, 'context': 0, 'coh_0': 15, 'coh_1': 50, 'choice': 2, 'correct': True}\n",
            "Trial 4 {'ground_truth': 1, 'other_choice': 2, 'context': 1, 'coh_0': 15, 'coh_1': 15, 'choice': 2, 'correct': False}\n",
            "Trial 5 {'ground_truth': 1, 'other_choice': 1, 'context': 0, 'coh_0': 15, 'coh_1': 15, 'choice': 2, 'correct': False}\n",
            "Trial 6 {'ground_truth': 2, 'other_choice': 1, 'context': 1, 'coh_0': 50, 'coh_1': 50, 'choice': 2, 'correct': True}\n",
            "Trial 7 {'ground_truth': 2, 'other_choice': 1, 'context': 1, 'coh_0': 15, 'coh_1': 5, 'choice': 2, 'correct': True}\n",
            "Trial 8 {'ground_truth': 1, 'other_choice': 2, 'context': 0, 'coh_0': 5, 'coh_1': 15, 'choice': 2, 'correct': False}\n",
            "Trial 9 {'ground_truth': 1, 'other_choice': 2, 'context': 0, 'coh_0': 50, 'coh_1': 15, 'choice': 2, 'correct': False}\n",
            "Trial 10 {'ground_truth': 2, 'other_choice': 1, 'context': 0, 'coh_0': 15, 'coh_1': 15, 'choice': 2, 'correct': True}\n",
            "Average performance = 0.47\n",
            "Performance: 94/200\n"
          ]
        }
      ]
    }
  ]
}