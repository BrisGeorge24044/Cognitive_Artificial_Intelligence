{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0sg2Fxjfai1IWrIl/wEAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrisGeorge24044/Cognitive_Artificial_Intelligence/blob/main/LeakyRNN_with_Hebbian_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hebbian Learning Rule:\n",
        "- Neurons that fire together, wire together\n",
        "\n",
        "    Δw<sub>ij</sub> = η ⋅ x<sub>i</sub> ⋅ y<sub>j</sub>\n",
        "\n",
        "- w<sub>ij</sub> = Weight which connects the pre- and post-synaptic neurons together\n",
        "- x<sub>i</sub> = pre-synaptic neuron activity\n",
        "- y<sub>j</sub> = post-synaptic neuron activity\n",
        "- η = Learning Rate\n"
      ],
      "metadata": {
        "id": "IkixReisQkoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neurogym\n",
        "!pip install gym==0.25.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI5dWlYkQrTR",
        "outputId": "43e08af5-1836-44cd-d526-52bd5ec9db53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neurogym\n",
            "  Downloading neurogym-0.0.2.tar.gz (79 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/79.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neurogym) (1.26.4)\n",
            "Collecting gym<0.25,>=0.20.0 (from neurogym)\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.4/696.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from neurogym) (3.8.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->neurogym) (1.16.0)\n",
            "Building wheels for collected packages: neurogym, gym\n",
            "  Building wheel for neurogym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neurogym: filename=neurogym-0.0.2-py3-none-any.whl size=118573 sha256=20eaa193e6285278dabc4544f5ff04f28bb530f6f33152493782733672e5d107\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/57/a7/66ed4eccf946052534253e4279438b97133b64facca56d4238\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793129 sha256=b76f9bce963ce41d26ad0e8e4a7bc4ad0e9184877db8e57cde3edd371f32daea\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/fb/19/388995b88cb551717a8dff40c889172cd12fadf994216a0a22\n",
            "Successfully built neurogym gym\n",
            "Installing collected packages: gym, neurogym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.24.1 neurogym-0.0.2\n",
            "Collecting gym==0.25.1\n",
            "  Downloading gym-0.25.1.tar.gz (732 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.2/732.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.25.1-py3-none-any.whl size=849026 sha256=7538d69ebba7747c11f00eb24fbb847098cf6e915e892997301b6e61aa8a3183\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/3b/df/78994c45c86a980cd5d8404c6d38cd28b871d5120e45c32ce4\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.24.1\n",
            "    Uninstalling gym-0.24.1:\n",
            "      Successfully uninstalled gym-0.24.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "neurogym 0.0.2 requires gym<0.25,>=0.20.0, but you have gym 0.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "t5pGpjUCVlFI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeakyRNN Class defines the Leaky RNN and sets parameters."
      ],
      "metadata": {
        "id": "syxcL33ygO0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeakyRNN(nn.Module):\n",
        "  # Function applies the hebbian learning rule\n",
        "  def hebbian_learning_rule(self, input, hidden, lr = 0.01):\n",
        "    batch_size = input.size(0)\n",
        "\n",
        "    # Add dimension for input and hidden\n",
        "    input = input.view(batch_size, 1, -1)\n",
        "    hidden = hidden.view(batch_size, -1, 1)\n",
        "\n",
        "    # Computes weight change for input to hidden state\n",
        "    # torch.bmm performs batch multiplication\n",
        "    # Creates (batch_size, hidden_size, input_size)\n",
        "    delta_w_i = lr * torch.bmm(hidden, input)\n",
        "    # Computes weight change for hidden to hidden state\n",
        "    # Creates (batch_size, hidden_size, hidden_size)\n",
        "    delta_w_j = lr * torch.bmm(hidden, hidden.transpose(1, 2))\n",
        "\n",
        "    # Weight changes are added together to update the weight matrices\n",
        "    delta_w_i_sum = delta_w_i.sum(dim=0)\n",
        "    delta_w_j_sum = delta_w_j.sum(dim=0)\n",
        "\n",
        "    # scale factor controls the impact of the learning rule\n",
        "    hebbian_scale_factor = 1\n",
        "\n",
        "    # torch.no_grad disables gradient tracking as it is not required for hebbian\n",
        "    # learning\n",
        "    with torch.no_grad():\n",
        "        # Updates the weights using the scale factor and the sum of the\n",
        "        # weight changes\n",
        "        self.input2hidden.weight += hebbian_scale_factor * delta_w_i_sum\n",
        "        self.hidden2hidden.weight += hebbian_scale_factor * delta_w_j_sum\n",
        "        # torch.norm normalises the weights. Previously there were stability\n",
        "        # issues which are now avoided. The addition of 1e-6 prevents division\n",
        "        # by zero.\n",
        "        self.input2hidden.weight.data = self.input2hidden.weight.data / (torch.norm(self.input2hidden.weight.data) + 1e-6)\n",
        "        self.hidden2hidden.weight.data = self.hidden2hidden.weight.data / (torch.norm(self.hidden2hidden.weight.data) + 1e-6)\n",
        "\n",
        "        # Created a check for NaN in weights as this was an issue which was\n",
        "        # causing instability\n",
        "    if torch.isnan(self.input2hidden.weight).any() or torch.isnan(self.hidden2hidden.weight).any():\n",
        "        print(\"NaN detected in weights!\")\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dt = None):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.tau = 100\n",
        "    # Sets alpha as a function of the time step, dt, and the time constant, tau\n",
        "    if dt is None:\n",
        "      alpha = 1\n",
        "    else:\n",
        "      alpha = dt/self.tau\n",
        "    self.alpha = alpha\n",
        "    # Defines linear layers for the input to hidden and hidden to hidden\n",
        "    # connections\n",
        "    self.input2hidden = nn.Linear(input_size, hidden_size)\n",
        "    self.hidden2hidden = nn.Linear(hidden_size, hidden_size)\n",
        "  # init_hidden function initialises the hidden state as a matrix of zeros\n",
        "  def init_hidden(self, input_shape):\n",
        "    batch_size = input_shape[1]\n",
        "    return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "  # Recurrence function updates the hidden state\n",
        "  def recurrence(self, input, hidden):\n",
        "    self.hebbian_learning_rule(input, hidden)\n",
        "    # ReLU non-linear activation function is used to calculate the new hidden\n",
        "    # state using the input and recurrent weight matrices\n",
        "    h_new = torch.relu(self.input2hidden(input) + self.hidden2hidden(hidden))\n",
        "    # Leaky integration is applied with alpha deciding how much of the last\n",
        "    # state is retained\n",
        "    h_new = hidden * (1 - self.alpha) + h_new * self.alpha\n",
        "    return h_new\n",
        "\n",
        "   # Forward function - process the sequence for every time step\n",
        "  def forward(self, input, hidden = None):\n",
        "    # If there is no hidden state, then it is initialised here\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(input.shape).to(input.device)\n",
        "\n",
        "      output = []\n",
        "      # Creates a loop for every time step\n",
        "      steps = range(input.size(0))\n",
        "      for i in steps:\n",
        "        # Updates the hidden state using the recurrence function\n",
        "        hidden = self.recurrence(input[i], hidden)\n",
        "        output.append(hidden)\n",
        "\n",
        "      # Stacks all outputs in one dimension\n",
        "      output = torch.stack(output, dim = 0)\n",
        "      return output, hidden"
      ],
      "metadata": {
        "id": "FGX28vPhVqS5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNNet Class creates a fully connected layer which converts the hidden states into final predictions."
      ],
      "metadata": {
        "id": "IpMh2Ma8gaFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
        "    super().__init__()\n",
        "    # The LeakyRNN class is instantiated\n",
        "    self.rnn = LeakyRNN(input_size, hidden_size, **kwargs)\n",
        "    # A fully connected layer is created and applied as the last step of the\n",
        "    # program to obtain predictions\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Forward function processes the input through the Leaky RNN and the FC layer\n",
        "  def forward(self, x):\n",
        "    rnn_output, _ = self.rnn(x)\n",
        "    # FC layer is applied to obtain the predictions for every time step\n",
        "    out = self.fc(rnn_output)\n",
        "    return out, rnn_output"
      ],
      "metadata": {
        "id": "Fegx423sV2oO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import neurogym as ngym\n",
        "\n",
        "# ContextDecisionMaking-v0 task is selected from neurogym\n",
        "task_name = 'ContextDecisionMaking-v0'\n",
        "# Time step and stimulus length are defined\n",
        "kwargs = {'dt': 20, 'timing': {'stimulus': 1500}}"
      ],
      "metadata": {
        "id": "4bqec8_KV40V"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The sequence length (Num of time steps for each input) and batch size are defined\n",
        "seq_len = 150\n",
        "batch_size = 24\n",
        "\n",
        "# Dataset is created with ngym.Dataset\n",
        "dataset = ngym.Dataset(task_name, env_kwargs=kwargs, batch_size=batch_size, seq_len=seq_len)\n",
        "env = dataset.env\n",
        "\n",
        "# Inputs and targets are taken from the dataset\n",
        "inputs, target = dataset()\n",
        "inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "\n",
        "# Input and output size are defined by the neurogym environment\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "\n",
        "print(inputs.shape)\n",
        "print(target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ1qCJT6V61B",
        "outputId": "ad1ce2ee-2647-4f89-f181-32b864fc182d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([150, 24, 7])\n",
            "(150, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines the hidden size\n",
        "hidden_size = 192\n",
        "\n",
        "# RNNNet is instantiated\n",
        "net = RNNNet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dt=env.dt)\n",
        "print(net)\n",
        "\n",
        "# This function trains the model using the Hebbian Learning Rule\n",
        "def train_model_hebbian(net, dataset):\n",
        "  # Defines the loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Tracks the loss and time\n",
        "  running_loss = 0\n",
        "  start_time = time.time()\n",
        "  # Creates a loop for the number of iterationa\n",
        "  for i in range(300):\n",
        "      # Takes inputs and labels from the dataset\n",
        "      inputs, labels = dataset()\n",
        "      inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "      labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
        "\n",
        "      # Creates a forward pass\n",
        "      output, _ = net(inputs)\n",
        "      # Tensor is flattened to 2D\n",
        "      output = output.view(-1, output_size)\n",
        "      # Loss is calcualated using the loss function\n",
        "      loss = criterion(output, labels)\n",
        "\n",
        "      # This tracks the loss from the loss at each iteration\n",
        "      running_loss += loss.item()\n",
        "      # If statement to calculate and print an average loss every 100\n",
        "      # iterations\n",
        "      if (i+1) % 10 == 0:\n",
        "          running_loss /= 10\n",
        "          print('Step {}, Loss {:0.4f}, Time {:0.1f}s'.format(\n",
        "              i+1, running_loss, time.time() - start_time))\n",
        "          running_loss = 0\n",
        "\n",
        "  return net\n",
        "\n",
        "\n",
        "net = train_model_hebbian(net, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mLTK3agV7Bs",
        "outputId": "38cb16ae-16e8-4fbd-a0f1-628b72c7e90c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNNet(\n",
            "  (rnn): LeakyRNN(\n",
            "    (input2hidden): Linear(in_features=7, out_features=192, bias=True)\n",
            "    (hidden2hidden): Linear(in_features=192, out_features=192, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=192, out_features=3, bias=True)\n",
            ")\n",
            "Step 10, Loss 3.9078, Time 2.8s\n",
            "Step 20, Loss 3.8962, Time 5.5s\n",
            "Step 30, Loss 3.8932, Time 8.4s\n",
            "Step 40, Loss 3.9108, Time 11.7s\n",
            "Step 50, Loss 3.9039, Time 14.4s\n",
            "Step 60, Loss 3.8973, Time 17.1s\n",
            "Step 70, Loss 3.9130, Time 20.0s\n",
            "Step 80, Loss 3.8988, Time 23.3s\n",
            "Step 90, Loss 3.8894, Time 26.1s\n",
            "Step 100, Loss 3.9075, Time 28.9s\n",
            "Step 110, Loss 3.8975, Time 31.6s\n",
            "Step 120, Loss 3.8839, Time 34.5s\n",
            "Step 130, Loss 3.9085, Time 37.8s\n",
            "Step 140, Loss 3.9048, Time 40.4s\n",
            "Step 150, Loss 3.8971, Time 43.2s\n",
            "Step 160, Loss 3.9128, Time 45.9s\n",
            "Step 170, Loss 3.8992, Time 49.2s\n",
            "Step 180, Loss 3.9027, Time 52.0s\n",
            "Step 190, Loss 3.8997, Time 54.8s\n",
            "Step 200, Loss 3.8974, Time 57.6s\n",
            "Step 210, Loss 3.8923, Time 60.5s\n",
            "Step 220, Loss 3.9221, Time 64.3s\n",
            "Step 230, Loss 3.9048, Time 67.4s\n",
            "Step 240, Loss 3.9010, Time 70.1s\n",
            "Step 250, Loss 3.9098, Time 72.9s\n",
            "Step 260, Loss 3.8955, Time 76.1s\n",
            "Step 270, Loss 3.8954, Time 79.1s\n",
            "Step 280, Loss 3.9078, Time 81.9s\n",
            "Step 290, Loss 3.8815, Time 84.6s\n",
            "Step 300, Loss 3.8933, Time 87.6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Phase:"
      ],
      "metadata": {
        "id": "l36eyeCGgmkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment is reset after training\n",
        "env = dataset.env\n",
        "env.reset(no_step=True)\n",
        "\n",
        "# Performance is tracked\n",
        "perf = 0\n",
        "# Dictionaries to store activity and information for each trial are created\n",
        "activity_dict = {}\n",
        "trial_infos = {}\n",
        "\n",
        "# Number of trials is set to 200\n",
        "num_trial = 200\n",
        "\n",
        "# For loop is created to run the model for 200 trials\n",
        "for i in range(num_trial):\n",
        "  # Starts a trial\n",
        "  trial_info = env.new_trial()\n",
        "  # defines the observations and ground truths for each trial\n",
        "  ob, gt = env.ob, env.gt\n",
        "  # Converts observations to batch dimensions\n",
        "  inputs = torch.from_numpy(ob[:,np.newaxis, :]).type(torch.float)\n",
        "\n",
        "  # Takes the predictions from the model\n",
        "  action_pred, rnn_activity = net(inputs)\n",
        "\n",
        "  # Converts the predictions to a numpy array and selects the action predicted\n",
        "  # by the model (the largest value)\n",
        "  action_pred = action_pred.detach().numpy()[:, 0, :]\n",
        "  choice = np.argmax(action_pred[-1, :])\n",
        "  correct = choice == gt[-1]\n",
        "\n",
        "  # Stores the activity and trial info in dictionaries\n",
        "  rnn_activity = rnn_activity[:, 0, :].detach().numpy()\n",
        "  activity_dict[i] = rnn_activity\n",
        "  trial_infos[i] = trial_info\n",
        "  trial_infos[i].update({'choice': choice, 'correct': correct})\n",
        "\n",
        "  # Tracks the performance\n",
        "  if correct:\n",
        "    perf += 1\n",
        "\n",
        "for i in range(20):\n",
        "  print('Trial', i + 1, trial_infos[i])\n",
        "\n",
        "print(f\"Average performance = {np.mean([val['correct'] for val in trial_infos.values()])}\")\n",
        "print(f\"Performance: {perf}/{num_trial}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y3kpc5BV7J_",
        "outputId": "87d8543e-5e49-4742-9afb-7be48126b7e0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 {'ground_truth': 1, 'other_choice': 2, 'context': 0, 'coh_0': 5, 'coh_1': 50, 'choice': 2, 'correct': False}\n",
            "Trial 2 {'ground_truth': 2, 'other_choice': 1, 'context': 0, 'coh_0': 15, 'coh_1': 50, 'choice': 2, 'correct': True}\n",
            "Trial 3 {'ground_truth': 2, 'other_choice': 2, 'context': 1, 'coh_0': 50, 'coh_1': 15, 'choice': 2, 'correct': True}\n",
            "Trial 4 {'ground_truth': 1, 'other_choice': 1, 'context': 1, 'coh_0': 50, 'coh_1': 15, 'choice': 2, 'correct': False}\n",
            "Trial 5 {'ground_truth': 1, 'other_choice': 1, 'context': 0, 'coh_0': 50, 'coh_1': 5, 'choice': 2, 'correct': False}\n",
            "Trial 6 {'ground_truth': 2, 'other_choice': 1, 'context': 1, 'coh_0': 15, 'coh_1': 15, 'choice': 2, 'correct': True}\n",
            "Trial 7 {'ground_truth': 2, 'other_choice': 2, 'context': 0, 'coh_0': 15, 'coh_1': 5, 'choice': 2, 'correct': True}\n",
            "Trial 8 {'ground_truth': 1, 'other_choice': 1, 'context': 1, 'coh_0': 15, 'coh_1': 50, 'choice': 2, 'correct': False}\n",
            "Trial 9 {'ground_truth': 2, 'other_choice': 2, 'context': 0, 'coh_0': 5, 'coh_1': 15, 'choice': 2, 'correct': True}\n",
            "Trial 10 {'ground_truth': 2, 'other_choice': 2, 'context': 1, 'coh_0': 5, 'coh_1': 5, 'choice': 2, 'correct': True}\n",
            "Trial 11 {'ground_truth': 1, 'other_choice': 2, 'context': 1, 'coh_0': 15, 'coh_1': 50, 'choice': 2, 'correct': False}\n",
            "Trial 12 {'ground_truth': 2, 'other_choice': 2, 'context': 0, 'coh_0': 15, 'coh_1': 5, 'choice': 2, 'correct': True}\n",
            "Trial 13 {'ground_truth': 2, 'other_choice': 2, 'context': 0, 'coh_0': 50, 'coh_1': 15, 'choice': 2, 'correct': True}\n",
            "Trial 14 {'ground_truth': 1, 'other_choice': 1, 'context': 1, 'coh_0': 50, 'coh_1': 15, 'choice': 2, 'correct': False}\n",
            "Trial 15 {'ground_truth': 2, 'other_choice': 2, 'context': 0, 'coh_0': 5, 'coh_1': 50, 'choice': 2, 'correct': True}\n",
            "Trial 16 {'ground_truth': 2, 'other_choice': 1, 'context': 1, 'coh_0': 15, 'coh_1': 50, 'choice': 2, 'correct': True}\n",
            "Trial 17 {'ground_truth': 2, 'other_choice': 2, 'context': 1, 'coh_0': 15, 'coh_1': 5, 'choice': 2, 'correct': True}\n",
            "Trial 18 {'ground_truth': 2, 'other_choice': 1, 'context': 1, 'coh_0': 50, 'coh_1': 15, 'choice': 2, 'correct': True}\n",
            "Trial 19 {'ground_truth': 1, 'other_choice': 2, 'context': 0, 'coh_0': 50, 'coh_1': 50, 'choice': 2, 'correct': False}\n",
            "Trial 20 {'ground_truth': 1, 'other_choice': 2, 'context': 0, 'coh_0': 5, 'coh_1': 5, 'choice': 2, 'correct': False}\n",
            "Average performance = 0.535\n",
            "Performance: 107/200\n"
          ]
        }
      ]
    }
  ]
}