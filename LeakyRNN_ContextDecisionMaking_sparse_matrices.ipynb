{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhF3fpCU5CYlhIgDGWhP4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrisGeorge24044/Cognitive_Artificial_Intelligence/blob/main/LeakyRNN_ContextDecisionMaking_sparse_matrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MT5ypecXCD4S",
        "outputId": "fb09ea85-f519-4002-de75-4609f4288376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neurogym in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neurogym) (1.26.4)\n",
            "Collecting gym<0.25,>=0.20.0 (from neurogym)\n",
            "  Using cached gym-0.24.1-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from neurogym) (3.8.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->neurogym) (1.16.0)\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.1\n",
            "    Uninstalling gym-0.25.1:\n",
            "      Successfully uninstalled gym-0.25.1\n",
            "Successfully installed gym-0.24.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              },
              "id": "faba65aa47874f8ebbbb648d6776f85f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.25.1\n",
            "  Using cached gym-0.25.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1) (0.0.8)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
            "    next(self.gen)\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 247, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2786, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3082, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3135, in __init__\n",
            "    super().__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 36, in __init__\n",
            "    parsed = _parse_requirement(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 62, in parse_requirement\n",
            "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 80, in _parse_requirement\n",
            "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 118, in _parse_requirement_details\n",
            "    specifier = _parse_specifier(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 208, in _parse_specifier\n",
            "    with tokenizer.enclosing_tokens(\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
            "    next(self.gen)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1740, in isEnabledFor\n",
            "    level >= self.getEffectiveLevel()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1710, in getEffectiveLevel\n",
            "    def getEffectiveLevel(self):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! pip install neurogym\n",
        "!pip install gym==0.25.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "2lmmkXvyCbxg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeakyRNN Class defines the Leaky RNN and sets parameters."
      ],
      "metadata": {
        "id": "ZM0OqA2_VjD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeakyRNN(nn.Module):\n",
        "\n",
        "  # Function creates sparse matrices\n",
        "  def sparse_matrices(self, in_feat, out_feat, sparsity = 0.8):\n",
        "    linear_layer = nn.Linear(in_feat, out_feat)\n",
        "    # Creates a binary mask to apply the sparsity\n",
        "    mask = (torch.rand(out_feat, in_feat) > sparsity).float()\n",
        "    linear_layer.weight.data = linear_layer.weight.data * mask\n",
        "    return linear_layer\n",
        "\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dt = None):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.tau = 100\n",
        "    # Sets alpha as a function of the time step, dt, and the time constant, tau\n",
        "    if dt is None:\n",
        "      alpha = 1\n",
        "    else:\n",
        "      alpha = dt/self.tau\n",
        "    self.alpha = alpha\n",
        "    # Defines sparse layers for the input to hidden and hidden to hidden\n",
        "    # connections\n",
        "    self.input2hidden = self.sparse_matrices(input_size, hidden_size)\n",
        "    self.hidden2hidden = self.sparse_matrices(hidden_size, hidden_size)\n",
        "\n",
        "  # init_hidden function initialises the hidden state as a matrix of zeros\n",
        "  def init_hidden(self, input_shape):\n",
        "    batch_size = input_shape[1]\n",
        "    return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "  # Recurrence function updates the hidden state\n",
        "  def recurrence(self, input, hidden):\n",
        "    # ReLU non-linear activation function is used to calculate the new hidden\n",
        "    # state using the input and recurrent weight matrices\n",
        "    h_new = torch.relu(self.input2hidden(input) + self.hidden2hidden(hidden))\n",
        "    # Leaky integration is applied with alpha deciding how much of the last\n",
        "    # state is retained\n",
        "    h_new = hidden * (1 - self.alpha) + h_new * self.alpha\n",
        "    return h_new\n",
        "\n",
        "  # Forward function - process the sequence for every time step\n",
        "  def forward(self, input, hidden = None):\n",
        "    # If there is no hidden state, then it is initialised here\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(input.shape).to(input.device)\n",
        "\n",
        "      output = []\n",
        "      # Creates a loop for every time step\n",
        "      steps = range(input.size(0))\n",
        "      for i in steps:\n",
        "        # Updates the hidden state using the recurrence function\n",
        "        hidden = self.recurrence(input[i], hidden)\n",
        "        output.append(hidden)\n",
        "\n",
        "      # Stacks all outputs in one dimension\n",
        "      output = torch.stack(output, dim = 0)\n",
        "      return output, hidden"
      ],
      "metadata": {
        "id": "87sXVnkLCdv_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
        "    super().__init__()\n",
        "    # The LeakyRNN class is instantiated\n",
        "    self.rnn = LeakyRNN(input_size, hidden_size, **kwargs)\n",
        "    # A fully connected layer is created and applied as the last step of the\n",
        "    # program to obtain predictions\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Forward function processes the input through the Leaky RNN and the FC layer\n",
        "  def forward(self, x):\n",
        "    rnn_output, _ = self.rnn(x)\n",
        "    # FC layer is applied to obtain the predictions for every time step\n",
        "    out = self.fc(rnn_output)\n",
        "    return out, rnn_output"
      ],
      "metadata": {
        "id": "tnrJTH1DChRI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import neurogym as ngym\n",
        "\n",
        "# 'ContextDecisionMaking-v0' task is selected from neurogym\n",
        "task_name = 'ContextDecisionMaking-v0'\n",
        "# Time step and stimulus length are defined\n",
        "kwargs = {'dt': 20, 'timing': {'stimulus': 1500}}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0vni5qzCjmQ",
        "outputId": "f452872f-311c-40a0-e444-1c9c273d3b37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:396: UserWarning: \u001b[33mWARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The sequence length (Num of time steps for each input) and batch size are defined\n",
        "seq_len = 150\n",
        "batch_size = 24\n",
        "\n",
        "# Dataset is created with ngym.Dataset\n",
        "dataset = ngym.Dataset(task_name, env_kwargs=kwargs, batch_size=batch_size, seq_len=seq_len)\n",
        "env = dataset.env\n",
        "\n",
        "# Inputs and targets are taken from the dataset\n",
        "inputs, target = dataset()\n",
        "inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "\n",
        "# Input and output size are defined by the neurogym environment\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "\n",
        "print(inputs.shape)\n",
        "print(target.shape)"
      ],
      "metadata": {
        "id": "2Nf3TDtyCmLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03204cea-0018-4660-fb68-9b42d475ea02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([150, 24, 7])\n",
            "(150, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines hidden size\n",
        "hidden_size = 192\n",
        "\n",
        "# RNNNet is instantiated\n",
        "net = RNNNet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dt=env.dt)\n",
        "print(net)\n",
        "\n",
        "# Function trains the model\n",
        "def train_model(net, dataset):\n",
        "    # Uses Adam optimiser\n",
        "    optimizer = optim.Adam(net.parameters(), lr = 0.01)\n",
        "    # Defines the loss function for the task\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Tracks the loss and time\n",
        "    running_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Sets a loop of 2000 training iterations\n",
        "    for i in range(2000):\n",
        "        inputs, labels = dataset()\n",
        "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "        labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
        "\n",
        "        # Gradients are reset to zero for each iteration\n",
        "        optimizer.zero_grad()\n",
        "        # Creates a forward pass\n",
        "        output, _ = net(inputs)\n",
        "        # Tensor is flattened to 2D\n",
        "        output = output.view(-1, output_size)\n",
        "        # Loss is calcualated using the loss function\n",
        "        loss = criterion(output, labels)\n",
        "        # Backward pass is done to calculate the gradients\n",
        "        loss.backward()\n",
        "        # The parameters are all updated\n",
        "        optimizer.step()\n",
        "\n",
        "        # This tracks the loss from the loss at each iteration\n",
        "        running_loss += loss.item()\n",
        "        # If statement to calculate and print an average loss every 100\n",
        "        # iterations\n",
        "        if i % 100 == 99:\n",
        "            running_loss /= 100\n",
        "            print('Step {}, Loss {:0.4f}, Time {:0.1f}s'.format(\n",
        "                i+1, running_loss, time.time() - start_time))\n",
        "            running_loss = 0\n",
        "    return net\n",
        "\n",
        "# Calls the train_model function to train the model\n",
        "net = train_model(net, dataset)"
      ],
      "metadata": {
        "id": "8N_iUD8wColv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f2b69c1-1ecd-430e-f69a-bcb0a5e469e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNNet(\n",
            "  (rnn): LeakyRNN(\n",
            "    (input2hidden): Linear(in_features=7, out_features=192, bias=True)\n",
            "    (hidden2hidden): Linear(in_features=192, out_features=192, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=192, out_features=3, bias=True)\n",
            ")\n",
            "Step 100, Loss 0.2425, Time 8.9s\n",
            "Step 200, Loss 0.0702, Time 17.7s\n",
            "Step 300, Loss 0.0379, Time 27.0s\n",
            "Step 400, Loss 0.0293, Time 39.0s\n",
            "Step 500, Loss 0.0258, Time 49.1s\n",
            "Step 600, Loss 0.0242, Time 60.3s\n",
            "Step 700, Loss 0.0233, Time 69.5s\n",
            "Step 800, Loss 0.0229, Time 79.4s\n",
            "Step 900, Loss 0.0226, Time 87.7s\n",
            "Step 1000, Loss 0.0224, Time 96.6s\n",
            "Step 1100, Loss 0.0223, Time 105.7s\n",
            "Step 1200, Loss 0.0221, Time 114.3s\n",
            "Step 1300, Loss 0.0221, Time 122.9s\n",
            "Step 1400, Loss 0.0221, Time 131.9s\n",
            "Step 1500, Loss 0.0219, Time 140.8s\n",
            "Step 1600, Loss 0.0219, Time 149.1s\n",
            "Step 1700, Loss 0.0219, Time 158.2s\n",
            "Step 1800, Loss 0.0221, Time 167.0s\n",
            "Step 1900, Loss 0.0220, Time 175.2s\n",
            "Step 2000, Loss 0.0219, Time 184.3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment is reset after training\n",
        "env = dataset.env\n",
        "env.reset(no_step=True)\n",
        "\n",
        "# Performance is tracked\n",
        "perf = 0\n",
        "# Dictionaries to store activity and information for each trial are created\n",
        "activity_dict = {}\n",
        "trial_infos = {}\n",
        "\n",
        "# Number of trials is set to 200\n",
        "num_trial = 200\n",
        "\n",
        "# For loop is created to run the model for 200 trials\n",
        "for i in range(num_trial):\n",
        "  # Starts a trial\n",
        "  trial_info = env.new_trial()\n",
        "  # defines the observations and ground truths for each trial\n",
        "  ob, gt = env.ob, env.gt\n",
        "  # Converts observations to batch dimensions\n",
        "  inputs = torch.from_numpy(ob[:,np.newaxis, :]).type(torch.float)\n",
        "\n",
        "  # Takes the predictions from the model\n",
        "  action_pred, rnn_activity = net(inputs)\n",
        "\n",
        "  # Converts the predictions to a numpy array and selects the action predicted\n",
        "  # by the model (the largest value)\n",
        "  action_pred = action_pred.detach().numpy()[:, 0, :]\n",
        "  choice = np.argmax(action_pred[-1, :])\n",
        "  correct = choice == gt[-1]\n",
        "\n",
        "  # Stores the activity and trial info in dictionaries\n",
        "  rnn_activity = rnn_activity[:, 0, :].detach().numpy()\n",
        "  activity_dict[i] = rnn_activity\n",
        "  trial_infos[i] = trial_info\n",
        "  trial_infos[i].update({'choice': choice, 'correct': correct})\n",
        "\n",
        "  # Tracks the performance\n",
        "  if correct:\n",
        "    perf += 1\n",
        "\n",
        "for i in range(10):\n",
        "  print('Trial', i, trial_infos[i])\n",
        "\n",
        "print(f\"Average performance = {np.mean([val['correct'] for val in trial_infos.values()])}\")\n",
        "print(f\"Performance: {perf}/{num_trial}\")"
      ],
      "metadata": {
        "id": "1mnIHXrtCq7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1052b8b9-6fdf-42a7-ba7c-c6ac6df1e960"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 {'ground_truth': 1, 'other_choice': 1, 'context': 1, 'coh_0': 50, 'coh_1': 50, 'choice': 1, 'correct': True}\n",
            "Trial 1 {'ground_truth': 1, 'other_choice': 1, 'context': 1, 'coh_0': 15, 'coh_1': 50, 'choice': 1, 'correct': True}\n",
            "Trial 2 {'ground_truth': 1, 'other_choice': 1, 'context': 1, 'coh_0': 50, 'coh_1': 50, 'choice': 1, 'correct': True}\n",
            "Trial 3 {'ground_truth': 2, 'other_choice': 2, 'context': 1, 'coh_0': 5, 'coh_1': 50, 'choice': 1, 'correct': False}\n",
            "Trial 4 {'ground_truth': 2, 'other_choice': 1, 'context': 1, 'coh_0': 50, 'coh_1': 50, 'choice': 1, 'correct': False}\n",
            "Trial 5 {'ground_truth': 1, 'other_choice': 1, 'context': 0, 'coh_0': 50, 'coh_1': 50, 'choice': 1, 'correct': True}\n",
            "Trial 6 {'ground_truth': 1, 'other_choice': 2, 'context': 1, 'coh_0': 50, 'coh_1': 50, 'choice': 1, 'correct': True}\n",
            "Trial 7 {'ground_truth': 2, 'other_choice': 2, 'context': 1, 'coh_0': 15, 'coh_1': 15, 'choice': 1, 'correct': False}\n",
            "Trial 8 {'ground_truth': 2, 'other_choice': 2, 'context': 1, 'coh_0': 50, 'coh_1': 5, 'choice': 1, 'correct': False}\n",
            "Trial 9 {'ground_truth': 1, 'other_choice': 2, 'context': 0, 'coh_0': 15, 'coh_1': 50, 'choice': 1, 'correct': True}\n",
            "Average performance = 0.49\n",
            "Performance: 98/200\n"
          ]
        }
      ]
    }
  ]
}